{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiple_linear_regression_stochastic_gradient_descent.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6Mgh9fjMOGwl+2F5wQAcd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gauravhulmukh/All_ml_algorithm_from_scratch/blob/master/Linear%20Regression/Multiple_linear_regression_stochastic_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2evcxL6O9wr",
        "colab_type": "text"
      },
      "source": [
        "**Multiple Linear Regression Stochastic Gradient Descent**<br>\n",
        "\n",
        "For a quick simple explanation:\n",
        "\n",
        "In both gradient descent (GD) and stochastic gradient descent (SGD), you update a set of parameters in an iterative manner to minimize an error function.\n",
        "\n",
        "While in GD, you have to run through ALL the samples in your training set to do a single update for a parameter in a particular iteration, in SGD, on the other hand, you use ONLY ONE or SUBSET of training sample from your training set to do the update for a parameter in a particular iteration. If you use SUBSET, it is called Minibatch Stochastic gradient Descent.\n",
        "\n",
        "Thus, if the number of training samples are large, in fact very large, then using gradient descent may take too long because in every iteration when you are updating the values of the parameters, you are running through the complete training set. On the other hand, using SGD will be faster because you use only one training sample and it starts improving itself right away from the first sample.\n",
        "\n",
        "SGD often converges much faster compared to GD but the error function is not as well minimized as in the case of GD. Often in most cases, the close approximation that you get in SGD for the parameter values are enough because they reach the optimal values and keep oscillating there.\n",
        "\n",
        "Zip Function = Used to combine two list.\n",
        "Enumerate function = used to indexing the list.\n",
        "Things need to be Remember:\n",
        "1. Shuffle the training examples\n",
        "2. Calculate Batch cost \n",
        "3. Avg. of batch cost will be get stored in Cost_history(array)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf-cjZx3RqqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "train_data = pd.read_csv('train_dataset.csv')\n",
        "test_data = pd.read_csv('test_dataset.csv')\n",
        "x = train_data[['a','b']].values\n",
        "y = train_data['y'].values\n",
        "x_test = test_data[['a','b']].values\n",
        "y_test = test_data['y'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_5TSRMpqC4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hypothesis structure(Linear Representation) y = theta0 + (theta1*x)\n",
        "def hypothesis(theta_coeff, x):\n",
        "    \"\"\"\n",
        "    theta_coeff(array): coefficients of function (1 x num_of_coeff)\n",
        "    x: input data/variable (num_of_samples x 1)\n",
        "    \n",
        "    Returns:\n",
        "    Predicted value for each example-row in inputs (1 x num_of_inputs)\n",
        "    \"\"\"\n",
        "    return x.dot(theta_coeff)\n",
        "\n",
        "def cost_function_mse(theta_coeff, x, y):\n",
        "    \"\"\"\n",
        "    theta_coeff(array): coefficients of function (1 x num_of_coeff)\n",
        "    x: input data/variable (num_of_samples x 1)\n",
        "    y: output data/variable (num_of_samples x 1)\n",
        "    \n",
        "    Returns:\n",
        "    Computes cost of using theta_coeff as coefficients for Linear Representation wrt to training data\n",
        "    \"\"\"\n",
        "    predicted_y = hypothesis(theta_coeff, x)\n",
        "    return np.sum((predicted_y - y)**2) / (2*x.shape[0])\n",
        "\n",
        "def compute_gradient(theta_coeff, x, y):\n",
        "    \"\"\"\n",
        "    theta_coeff(array): coefficients of function (1 x num_of_coeff)\n",
        "    x: input data/variable (num_of_samples x 1)\n",
        "    y: output data/variable (num_of_samples x 1)\n",
        "    \n",
        "    Returns:\n",
        "    Array of computed gradients for each of the coefficients (1 x num_of_coefficients)\n",
        "    \"\"\"\n",
        "    predicted_y = hypothesis(theta_coeff, x)\n",
        "    # coeff_gradients = np.sum(((predicted_y - y)**2) * x.T, axis=1) / (x.shape[0])\n",
        "    coeff_gradients = np.sum((predicted_y - y) * x.T, axis=1) / (x.shape[0])\n",
        "    return coeff_gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-WMQIHewfeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stochastic_gradient_descent(x,y,iterations,learning_rate,batch_size):\n",
        "  '''\n",
        "  x : Input data/variables (num_of_samples X 1)\n",
        "  y : Output data/variables (num_of_samples X 1)\n",
        "  iterations(int) : Number_of_iterations/epochs\n",
        "  learning_rate(int) : Alpha / learning rate which controls the descent\n",
        "\n",
        "  Return:\n",
        "  theta_coeff(array) : Best set of coefficients for the linear model(1 X num_of_coefficient)\n",
        "  cost_history(array) : Cost history wrt coefficients computed while performing the Descent.\n",
        "  '''\n",
        "  cost_history = []\n",
        "  theta_coeff = np.array([0] * x.shape[1])\n",
        "  #shuffle the training examples\n",
        "  #shuffles ensure that each data point create independent change on the model, without being biased by the same points before them\n",
        "  #shuffling mini-batchs make the gradients more variable, which can help convergence because it increases the likelihood of hitting a good direction\"\n",
        "  shuffled_indices = np.arange(x.shape[0])\n",
        "  np.random.shuffle(shuffled_indices)\n",
        "  x = x[shuffled_indices]\n",
        "  y = y[shuffled_indices]\n",
        "  for iteration in range(iterations):\n",
        "        batch_cost = []\n",
        "        for training_example_idx, (training_example_x, training_example_y) in enumerate(zip(x,y)):\n",
        "            # Predict -> compute cost -> compute gradient ... wrt to each example\n",
        "            example_x = training_example_x.reshape([-1, x.shape[1]])\n",
        "            coeff_gradients = compute_gradient(theta_coeff,example_x,training_example_y)\n",
        "            # Alter old theta_coeff using movement suggested by new coeff_gradients\n",
        "            theta_coeff = theta_coeff - (learning_rate * coeff_gradients)\n",
        "            # Compute the cost using theta_coeff as coefficients of linear function\n",
        "            cost = cost_function_mse(theta_coeff, x, y)\n",
        "            batch_cost.append(cost)\n",
        "            # Append the cost to cost_history by averaging the accumulated costs in batch_cost_container\n",
        "            if len(batch_cost) % batch_size == 0:\n",
        "                cost_history.append(np.mean(batch_cost))\n",
        "                batch_cost = []\n",
        "  return theta_coeff, cost_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLipvYl4onOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Prepare data\n",
        "# Learning rate\n",
        "alpha = 0.0001\n",
        "# Bias variable \n",
        "bias_variable = np.ones(len(x))\n",
        "# Plug input along with bias_variable\n",
        "x_train = np.column_stack([bias_variable, x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WbB8NvQqfBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_theta_coeff, cost_history = stochastic_gradient_descent(x_train, y, 150, alpha, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7CPG3PJq22j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91f4522d-ab0c-4c69-dc07-adbfcd567dbc"
      },
      "source": [
        "cost_history[-1]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235.52104007912084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixQuHie_rAJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "4b97e481-098d-49aa-f0d3-0c1bb99e9ed7"
      },
      "source": [
        "plt.plot(cost_history)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f837fc7cac8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52V0TgwxrClY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(theta_coeff, x_input):\n",
        "    # Bias variable \n",
        "    bias_variable = np.ones(len(x_input))\n",
        "    # Plug input along with bias_variable\n",
        "    new_x_input = np.column_stack([bias_variable, x_input])\n",
        "    # Compute output values with new coefficients\n",
        "    preds = hypothesis(theta_coeff, new_x_input)\n",
        "    return preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfeBYKaMrKPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RMSE\n",
        "def rmse(y, y_pred):\n",
        "    rmse = np.sqrt(np.sum((y - y_pred)**2) / len(y))\n",
        "    return rmse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdisBCd0rMxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#R2 Score\n",
        "# How much(%) of the total variation in y is explained by variation in x(fitted line)\n",
        "def r2_score(y, y_pred):  \n",
        "    mean_y = np.mean(y)\n",
        "    SE_total_variation = np.sum((y - mean_y)**2) # Unexplained max possible variation in y wrt->Mean\n",
        "    SE_line_variation = np.sum((y - y_pred)**2) # Unexplained variation in y wrt -> fitted line\n",
        "    r2 = 1 - (SE_line_variation / SE_total_variation) # Expalined = 1 - Unexplained\n",
        "    return r2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrhv7h06rUld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_train = get_predictions(best_theta_coeff, x)#Training\n",
        "y_pred_test = get_predictions(best_theta_coeff, x_test)#Testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojRBnaOpriuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a880fbf5-b265-4491-85d0-dfc0cec0edfb"
      },
      "source": [
        "print(\"Training set evaluation\")\n",
        "print(f\"RMSE = {rmse(y, y_pred_train)}\")\n",
        "print(f\"R2_score = {r2_score(y, y_pred_train)}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set evaluation\n",
            "RMSE = 21.703504248650074\n",
            "R2_score = 0.9541415628392439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roCu-wR_rpUK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d210b573-69b8-496f-e8c6-65deb170b8dc"
      },
      "source": [
        "print(\"Testing set evaluation\")\n",
        "print(f\"RMSE = {rmse(test_data['y'].values, y_pred_test)}\")\n",
        "print(f\"R2_score = {r2_score(test_data['y'].values, y_pred_test)}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing set evaluation\n",
            "RMSE = 22.883321194027097\n",
            "R2_score = 0.9536679673246458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDSWRzOGtJfO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}